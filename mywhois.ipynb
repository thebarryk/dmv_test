{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to accumulate whois records and risk factors.\n",
    "They will memoized to reduce requested services for more data.\n",
    "\n",
    "1. Use a database to store the results for use in various programs.\n",
    "2. Use an ipnetwork as key\n",
    "3. Store company name with key\n",
    "4. When a class instance is created, it will load db into dictionary.\n",
    "5. The instance is normally readonly, but can be writeable. That means new addresses\n",
    "will be added if they are not in the database and become permanent parts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "risk.find: ip_string='142.255.122.114'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_2754/1566396705.py\u001b[0m(219)\u001b[0;36mfind\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    217 \u001b[0;31m        \u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"risk.find: {ip_string=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    218 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 219 \u001b[0;31m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    220 \u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipaddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    221 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2754/1566396705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_ip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mrisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0mafter\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2754/1566396705.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, ip_string)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"risk.find: {ip_string=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipaddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2754/1566396705.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, ip_string)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"risk.find: {ip_string=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipaddress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mip_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dmv_test/lib/python3.9/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dmv_test/lib/python3.9/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipaddress\n",
    "import dbm\n",
    "import pickle\n",
    "from sortedcontainers import SortedDict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "import datetime\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "class Debug():\n",
    "    \"\"\" Print debug messages if active \"\"\"\n",
    "    def __init__(self, set=1):\n",
    "        self._set = set\n",
    "    def prt(self, str):\n",
    "        if self._set:\n",
    "            sys.stderr.write(str)\n",
    "    def set(self):\n",
    "        self._set = 1\n",
    "    def unset(self):\n",
    "        self._set = 0\n",
    "debug = Debug()\n",
    "\n",
    "\n",
    "def get_risk(ip_string):\n",
    "    # Return risk factors from scamalytics into a dict\n",
    "    #     {\"ip\": ? , \n",
    "    #      \"score\": ?, \n",
    "    #      \"risk\": ?, \n",
    "    #      \"risk_comment: ?\"}\n",
    "\n",
    "    # Fetch the complete record from scamalytics restful api\n",
    "    # ip_string ... make request by ip address as a string\n",
    "\n",
    "    html_text = \"\"\n",
    "    url = \"https://scamalytics.com/ip/\" + ip_string\n",
    "    html_text = requests.get(url).text\n",
    "\n",
    "#     soup = BeautifulSoup(html_text, 'lxml')\n",
    "    soup = BeautifulSoup(html_text, 'xml')\n",
    "    \n",
    "    # Tag=pre\n",
    "    result = json.loads(soup.pre.string)\n",
    "    \n",
    "    # The comment is in the body of an unlabelled div. Used the css class to find.\n",
    "    # Remove special UTF-8 character \\U200b, a zero width space.\n",
    "#     result[\"risk_comment\"] = soup.find_all(\"div\", class_=\"panel_body\")[0].get_text().replace(\"\\u200b\",\"\")\n",
    "    result[\"risk_comment\"] = \"\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def parse_arin(html_text, ip_string):\n",
    "    # Values that are not found are set to np.NaN\n",
    "    \n",
    "    fillna = lambda x: np.nan if x is None else x if isinstance(x, str) else x.string\n",
    "    \n",
    "    def get_streetaddress(soup):\n",
    "        # More than one address line may be recorded\n",
    "        tag = soup.org.streetaddress\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        address = []\n",
    "        for line in tag:\n",
    "            address.append(line.string)\n",
    "        return address\n",
    "    def get_postalcode(soup):\n",
    "        return fillna(soup.org.postalcode)\n",
    "    def get_city(soup):\n",
    "        return fillna(soup.org.city)\n",
    "    def get_handle(soup):\n",
    "        return fillna(soup.org.handle)\n",
    "    def get_state(soup):\n",
    "        # The iso3166 tags are the international country codes\n",
    "        # Ref: https://www.iso.org/glossary-for-iso-3166.html\n",
    "        # BeautifulSoup does not parse tags contain \"-\" so\n",
    "        # use find_all to locate the tags with a string search.\n",
    "        tag = soup.org.find_all(\"iso3166-2\")\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        for t in tag:\n",
    "            x = fillna(t)\n",
    "        return x\n",
    "    def get_country(soup):\n",
    "        tag = soup.org.find_all(\"iso3166-1\")\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        for t in tag:\n",
    "            x = fillna(t.find('name'))\n",
    "        return x\n",
    "    def get_countrycode(soup):\n",
    "        tag = soup.org.find_all(\"iso3166-1\")\n",
    "        for t in tag:\n",
    "            x = fillna(t.code2)\n",
    "        return x\n",
    "    def get_organization(soup, info):\n",
    "        # There are 2 tag=name in the tag=org, one for country and one for organization. \n",
    "        # The country is part of iso3166-1 so it can be isolated. Look for organization\n",
    "        # by looking at both tags and selecting the one that is not equal to country.\n",
    "        for t in soup.org.find_all(\"name\"):\n",
    "            if t.string != info[\"country\"]:\n",
    "                x = fillna(t)\n",
    "        return t\n",
    "    \n",
    "    # Parse into dict to return results, item by item\n",
    "    result = {}\n",
    "\n",
    "    # Parse html into a hierarchy using BeautifulSoup \n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    # ARIN reports a list of CIDR net_addresses. \n",
    "    # The database will be indexed by ipaddress.net_address.\n",
    "    # A record will be written for each cidr and duplicate the ARIN info\n",
    "    # Obtain the organization name from tag=net instead of the tag=org which\n",
    "    # has more than one tag=name making it harder to isolate.\n",
    "    try:\n",
    "        info = {}\n",
    "\n",
    "        # Obtain info from tag=org\n",
    "\n",
    "        info[\"address\"] = get_streetaddress(soup)\n",
    "        info[\"postalcode\"] = get_postalcode(soup)\n",
    "        info[\"state\"] = get_state(soup)\n",
    "        info[\"country\"] = get_country(soup)\n",
    "        info[\"countrycode\"] = get_countrycode(soup)\n",
    "        info[\"organization\"] = get_organization(soup, info)\n",
    "        info[\"city\"] = get_city(soup)\n",
    "        info[\"handle\"] = get_handle(soup)\n",
    "\n",
    "        # Add the risk obtained from scamalytics\n",
    "#         info.update(get_risk(ip_string))\n",
    "\n",
    "        # The netblocks scope contains a list of netblock sections\n",
    "        for netblock in soup.net.netblocks:\n",
    "            cidr = netblock.startaddress.string + \"/\" + netblock.cidrlength.string      \n",
    "            result[cidr] = info\n",
    "        return result\n",
    "    \n",
    "    except:\n",
    "        print(f\"Error in parse_arin({ip_string=}\")\n",
    "        print(f\"{html_text=}\")\n",
    "        return None\n",
    "\n",
    "def get_arin(ip_string):\n",
    "    '''Return dict for the net_address that contains this ip_string\n",
    "        {\"cidr\": ?,\n",
    "         {\"organization\": ? ,\n",
    "          \"handle\": ? ,\n",
    "          \"city\": ? ,\n",
    "          \"address\" : ? ,\n",
    "          \"postalcode\": ? ,\n",
    "          \"countrycode\": ? ,\n",
    "          \"state\": ? ,\n",
    "          \"country\": ? ,\n",
    "          \"datetime\": ?,\n",
    "         }\n",
    "    '''\n",
    "\n",
    "    # Fetch the complete record from arin restful api\n",
    "    # Ref: https://www.arin.net/resources/registry/whois/rws/api/#networks-and-asns\n",
    "    # ip_string ... make request by ip address as a string\n",
    "    # pft ......... get full record\n",
    "    \n",
    "    url = \"http://whois.arin.net/rest/ip/\" + ip_string + \"/pft\"\n",
    "    html_text = \"\"\n",
    "    try:\n",
    "        html_text = requests.get(url).text\n",
    "    except:\n",
    "        return None\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return parse_arin(html_text, ip_string)\n",
    "\n",
    "class Risk():\n",
    "    \n",
    "    def __init__(self, filename, readonly=True):\n",
    "\n",
    "        # Open database. Create as needed.\n",
    "        \n",
    "        self.readonly = readonly\n",
    "        self.open_option = f'{\"r\" if self.readonly else \"w\"}'\n",
    "        self.db_filename = filename\n",
    "        self.hp = pickle.HIGHEST_PROTOCOL\n",
    "         \n",
    "        try:\n",
    "            self.db = dbm.open(self.db_filename, self.open_option)\n",
    "        except:\n",
    "            if self.readonly:\n",
    "                print(f\"{self.db_filename} does not exist but will not be created when class is {readonly=}\")\n",
    "                return None\n",
    "            else:\n",
    "                self.db = dbm.open(self.db_filename, \"c\")\n",
    "                \n",
    "        # Read the data into dictionary:\n",
    "        #   risk[ipaddress.ipv4network] = [organization, country, risk]\n",
    "        #   \n",
    "        \n",
    "        self.risk = SortedDict()\n",
    "        self.risk_count = 0\n",
    "\n",
    "        for key in self.db.keys():\n",
    "            self.risk[pickle.loads(key)] = pickle.loads(self.db[key])\n",
    "\n",
    "        self.risk_count = len(self.risk)\n",
    "        self.db.close()\n",
    "\n",
    "        \n",
    "    def find(self, ip_string):\n",
    "        \"\"\" \n",
    "        risk[cidr] = [company, location, risk_score]\n",
    "        creating one if needed and adding it to the database.\n",
    "        \"\"\"\n",
    "        debug.prt(f\"risk.find: {ip_string=}\")\n",
    "#         import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            self.ip = ipaddress.ip_address(ip_string)\n",
    "        except:\n",
    "            print(f\"Could not find IPv4Address for {ip_string}\")\n",
    "            return None\n",
    "        \n",
    "        # Find the address to insert\n",
    "        if self.cidr_search(self.ip):\n",
    "            return self.cidr_search_result\n",
    "        else:\n",
    "            self.result = get_arin(ip_string)\n",
    "            if self.result == None:\n",
    "                debug.prt(f\"No arin results for {ip_string=}\\n\")\n",
    "                return False\n",
    "            if not self.add(self.result):\n",
    "                return False\n",
    "            return self.result\n",
    "    \n",
    "\n",
    "    def add(self, new_risks):\n",
    "        # Add the result of get_arin, a dict with cidr as key \n",
    "        # to both the Risk.risk dict and the database\n",
    "        \n",
    "        # Store in dictionary first.\n",
    "        # There may be more than one cidr retrieved by get_arin\n",
    "        # Each CIDR has to be type ip_network\n",
    "\n",
    "        for new_cidr, new_risk in new_risks.items():\n",
    "            netblock = ipaddress.ip_network(new_cidr)\n",
    "            self.risk[netblock] = new_risk\n",
    "        \n",
    "        # Store in database next\n",
    "        if not self.readonly:\n",
    "            \n",
    "            # Collect the pickle's of each netblock and risk\n",
    "            additions = []\n",
    "#             import pdb; pdb.set_trace()\n",
    "            for new_cidr, new_risk in new_risks.items():\n",
    "                netblock = ipaddress.ip_network(new_cidr)\n",
    "                # key and value have to be pickle'd before storing\n",
    "                try:\n",
    "                    new_risk_temp = f\"{new_risk}\"\n",
    "                    pickled_netblock = pickle.dumps(netblock, protocol=self.hp)\n",
    "                    pickled_risk     = pickle.dumps(new_risk_temp, protocol=self.hp)\n",
    "                    additions.append([pickled_netblock, pickled_risk])\n",
    "                except BaseException as ex:\n",
    "                    print(ex)\n",
    "                    debug.prt(f\"Pickle error: {new_cidr=}\\n{new_risk=}\\n\")\n",
    "                    return False\n",
    "\n",
    "            # Write into database making sure to close it\n",
    "            with dbm.open(self.db_filename, self.open_option) as self.db:\n",
    "                for addition in additions:\n",
    "                    self.db[addition[0]] = addition[1]\n",
    "\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def len(self):\n",
    "        return self.risk_count\n",
    "\n",
    "\n",
    "    def cidr_search(self, target_ip):\n",
    "        # risk.cidr_search(target_ip) is True when ip's network is in db\n",
    "        # type(target_ip) is ipaddress.IPv4Address\n",
    "        # Updates cidr_search_result property with risk[cidr of target_ip] else None\n",
    "        sz = len(self.risk)\n",
    "        if sz == 0:\n",
    "            self.bisearch_result = None\n",
    "            return False\n",
    "        s = 0\n",
    "        e = sz\n",
    "        while s > e:\n",
    "            m = (s + e)//2\n",
    "            cidr = self.risk.peekitem(m)[0]\n",
    "            if target_ip in cidr:\n",
    "                self.cidr_search_result = self.risk[cidr]\n",
    "                return True\n",
    "            if target_ip > cidr:\n",
    "                s = m + 1\n",
    "            else:\n",
    "                e = m - 1\n",
    "        self.cidr_search_result = None\n",
    "        return False\n",
    "\n",
    "\n",
    "# Routine to read a clean set ip addresses from the sample data\n",
    "# and icorporate them into the risk database\n",
    "\n",
    "db_filename = \"mywhois\"\n",
    "sample_filename = \"clean_test_data.csv\"\n",
    "\n",
    "# Open the database and load the current data\n",
    "risk = Risk(db_filename, readonly=False)\n",
    "\n",
    "# Read the clean set of sample data set\n",
    "clean_ip = pd.read_csv(sample_filename)\n",
    "                       \n",
    "# range over the unique ip addresses\n",
    "new = 0\n",
    "old = 0\n",
    "for n, ip in enumerate(clean_ip.ip.drop_duplicates()[:10]):\n",
    "    before = risk.len()\n",
    "    risk.find(ip)\n",
    "    after  = risk.len()\n",
    "    if before == after:\n",
    "        old += 1\n",
    "    else:\n",
    "        new += 1\n",
    "print(f\"{n=} {new=} {old=} {risk.len()=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_arin(ip_string):\n",
    "    url       = \"http://whois.arin.net/rest/ip/\" + ip_string + \"/pft\"\n",
    "    html_text = \"\"\n",
    "    try:\n",
    "        html_text = requests.get(url).text\n",
    "    except:\n",
    "        return None\n",
    "    # Parse html into a hierarchy using BeautifulSoup \n",
    "#     soup = BeautifulSoup(html_text, 'lxml')\n",
    "    soup = BeautifulSoup(html_text, 'xml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = api_arin(\"24.228.215.103\")\n",
    "print(f\"{len(soup)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.netblocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to read a clean set ip addresses from the sample data\n",
    "# and icorporate them into the resk database\n",
    "\n",
    "db_filename = \"mywhois\"\n",
    "sample_filename = \"clean_test_data.csv\"\n",
    "\n",
    "# Open the database and load the current data\n",
    "risk = Risk(db_filename, readonly=False)\n",
    "\n",
    "# Read the clean set of sample data set\n",
    "clean_ip = pd.read_csv(sample_filename)\n",
    "                       \n",
    "# range over the unique ip addresses\n",
    "new = 0\n",
    "old = 0\n",
    "for n, ip in enumerate(clean_ip.ip.drop_duplicates()[2:3]):\n",
    "    before = risk.len()\n",
    "    risk.find(ip)\n",
    "    after  = risk.len()\n",
    "    if before == after:\n",
    "        old += 1\n",
    "    else:\n",
    "        new += 1\n",
    "print(f\"{n=} {new=} {old=} {risk.len()=}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk=Risk(\"mywhois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.255.122.114\n",
      "172.58.230.193\n",
      "72.68.212.63\n",
      "172.100.125.174\n",
      "24.228.215.103\n",
      "68.199.195.147\n",
      "108.29.95.66\n",
      "32.208.115.88\n",
      "24.189.68.18\n",
      "172.58.235.43\n"
     ]
    }
   ],
   "source": [
    "for ip in clean_ip.ip.drop_duplicates()[:10]:\n",
    "    print(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work={'organization': 'MCI Communications Services, Inc. d/b/a Verizon Business', 'handle': 'MCICS', 'asn': '', 'city': 'Ashburn', 'address': ['22001 Loudoun County Pkwy'], 'postalcode': '20147', 'state': 'VA', 'country': 'United States', 'ip': '72.68.212.63', 'score': '21', 'risk': 'medium', 'risk_comment': 'IP address 72.68.212.63 is operated by Verizon Communications whose web traffic we consider to present a potentially medium fraud risk. This IP address is owned by MCI Communications Services, Inc. d/b/a Verizon Business whose web traffic we also consider to present a potentially medium fraud risk. In both cases, non-web traffic may present a different risk or no risk at all. Scamalytics see low levels of traffic from Verizon Communications across our global network, little of which we suspect to be potentially fraudulent. We have no visibility into the web traffic directly from 72.68.212.63, and therefore apply a risk score of 21/100 based on the overall risk from Verizon Communications’s IP addresses where we do have visibility.'}\n",
    "pickle.dumps(f\"{work}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in risk.risk:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2754/474661594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# s.org\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iso3166-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "# t=requests.get(\"http://whois.arin.net/rest/ip/\" + ip_string + \"/pft\").text\n",
    "\n",
    "# s = BeautifulSoup(t)\n",
    "\n",
    "# s.org\n",
    "\n",
    "# for x in s.org.find_all(\"iso3166-1\"):\n",
    "#     country = x.find('name').string\n",
    "# print(country)\n",
    "\n",
    "# for x in s.org.find_all(\"name\"):\n",
    "#     if x.string != country:x\n",
    "#         organization = x.string\n",
    "\n",
    "\n",
    "# print(organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arin(html_text, ip_string):\n",
    "    # Values that are not found are set to np.NaN\n",
    "    \n",
    "    fillna = lambda x: np.nan if x is None else x if isinstance(x, str) else x.string\n",
    "    \n",
    "    def get_streetaddress(soup):\n",
    "        # More than one address line may be recorded\n",
    "        tag = soup.org.streetaddress\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        address = []\n",
    "        for line in tag:\n",
    "            address.append(line.string)\n",
    "        return address\n",
    "    def get_postalcode(soup):\n",
    "        return fillna(soup.org.postalcode)\n",
    "    def get_city(soup):\n",
    "        return fillna(soup.org.city)\n",
    "    def get_handle(soup):\n",
    "        return fillna(soup.org.handle)\n",
    "    def get_state(soup):\n",
    "        # The iso3166 tags are the international country codes\n",
    "        # Ref: https://www.iso.org/glossary-for-iso-3166.html\n",
    "        # BeautifulSoup does not parse tags contain \"-\" so\n",
    "        # use find_all to locate the tags with a string search.\n",
    "        tag = soup.org.find_all(\"iso3166-2\")\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        for t in tag:\n",
    "            x = fillna(t)\n",
    "        return x\n",
    "    def get_country(soup):\n",
    "        tag = soup.org.find_all(\"iso3166-1\")\n",
    "        if tag is None:\n",
    "            return np.nan\n",
    "        for t in tag:\n",
    "            x = fillna(t.find('name'))\n",
    "        return x\n",
    "    def get_countrycode(soup):\n",
    "        tag = soup.org.find_all(\"iso3166-1\")\n",
    "        for t in tag:\n",
    "            x = fillna(t.code2)\n",
    "        return x\n",
    "    def get_organization(soup, info):\n",
    "        # There are 2 tag=name in the tag=org, one for country and one for organization. \n",
    "        # The country is part of iso3166-1 so it can be isolated. Look for organization\n",
    "        # by looking at both tags and selecting the one that is not equal to country.\n",
    "        for t in soup.org.find_all(\"name\"):\n",
    "            if t.string != info[\"country\"]:\n",
    "                x = fillna(t)\n",
    "        return t\n",
    "    \n",
    "    # Parse into dict to return results, item by item\n",
    "    result = {}\n",
    "\n",
    "    # Parse html into a hierarchy using BeautifulSoup \n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    # ARIN reports a list of CIDR net_addresses. \n",
    "    # The database will be indexed by ipaddress.net_address.\n",
    "    # A record will be written for each cidr and duplicate the ARIN info\n",
    "    # Obtain the organization name from tag=net instead of the tag=org which\n",
    "    # has more than one tag=name making it harder to isolate.\n",
    "    try:\n",
    "        info = {}\n",
    "\n",
    "        # Obtain info from tag=org\n",
    "\n",
    "        info[\"address\"] = get_streetaddress(soup)\n",
    "        info[\"postalcode\"] = get_postalcode(soup)\n",
    "        info[\"state\"] = get_state(soup)\n",
    "        info[\"country\"] = get_country(soup)\n",
    "        info[\"countrycode\"] = get_countrycode(soup)\n",
    "        info[\"organization\"] = get_organization(soup, info)\n",
    "        info[\"city\"] = get_city(soup)\n",
    "        info[\"handle\"] = get_handle(soup)\n",
    "\n",
    "        # Add the risk obtained from scamalytics\n",
    "#         info.update(get_risk(ip_string))\n",
    "\n",
    "        # The netblocks scope contains a list of netblock sections\n",
    "        for netblock in soup.net.netblocks:\n",
    "            cidr = netblock.startaddress.string + \"/\" + netblock.cidrlength.string      \n",
    "            result[cidr] = info\n",
    "        return result\n",
    "    \n",
    "    except:\n",
    "        print(f\"Error in parse_arin({ip_string=}\")\n",
    "        print(f\"{html_text=}\")\n",
    "        return None\n",
    "\n",
    "def xget_arin(ip_string):\n",
    "    '''Return dict for the net_address that contains this ip_string\n",
    "        {\"cidr\": ?,\n",
    "         {\"organization\": ? ,\n",
    "          \"handle\": ? ,\n",
    "          \"city\": ? ,\n",
    "          \"address\" : ? ,\n",
    "          \"postalcode\": ? ,\n",
    "          \"countrycode\": ? ,\n",
    "          \"state\": ? ,\n",
    "          \"country\": ? ,\n",
    "         }\n",
    "    '''\n",
    "\n",
    "    # Fetch the complete record from arin restful api\n",
    "    # Ref: https://www.arin.net/resources/registry/whois/rws/api/#networks-and-asns\n",
    "    # ip_string ... make request by ip address as a string\n",
    "    # pft ......... get full record\n",
    "    \n",
    "    url = \"http://whois.arin.net/rest/ip/\" + ip_string + \"/pft\"\n",
    "    html_text = \"\"\n",
    "    try:\n",
    "        html_text = requests.get(url).text\n",
    "    except:\n",
    "        return None\n",
    "#     import pdb; pdb.set_trace()\n",
    "    return parse_arin(html_text, ip_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'72.65.128.0/17': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                    'city': 'Ashburn',\n",
      "                    'country': 'United States',\n",
      "                    'countrycode': 'US',\n",
      "                    'handle': 'MCICS',\n",
      "                    'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                    'postalcode': '20147',\n",
      "                    'state': 'VA'},\n",
      " '72.66.0.0/15': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                  'city': 'Ashburn',\n",
      "                  'country': 'United States',\n",
      "                  'countrycode': 'US',\n",
      "                  'handle': 'MCICS',\n",
      "                  'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                  'postalcode': '20147',\n",
      "                  'state': 'VA'},\n",
      " '72.68.0.0/15': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                  'city': 'Ashburn',\n",
      "                  'country': 'United States',\n",
      "                  'countrycode': 'US',\n",
      "                  'handle': 'MCICS',\n",
      "                  'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                  'postalcode': '20147',\n",
      "                  'state': 'VA'},\n",
      " '72.70.0.0/16': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                  'city': 'Ashburn',\n",
      "                  'country': 'United States',\n",
      "                  'countrycode': 'US',\n",
      "                  'handle': 'MCICS',\n",
      "                  'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                  'postalcode': '20147',\n",
      "                  'state': 'VA'},\n",
      " '72.71.0.0/17': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                  'city': 'Ashburn',\n",
      "                  'country': 'United States',\n",
      "                  'countrycode': 'US',\n",
      "                  'handle': 'MCICS',\n",
      "                  'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                  'postalcode': '20147',\n",
      "                  'state': 'VA'},\n",
      " '72.71.128.0/18': {'address': ['22001 Loudoun County Pkwy'],\n",
      "                    'city': 'Ashburn',\n",
      "                    'country': 'United States',\n",
      "                    'countrycode': 'US',\n",
      "                    'handle': 'MCICS',\n",
      "                    'organization': <name>MCI Communications Services, Inc. d/b/a Verizon Business</name>,\n",
      "                    'postalcode': '20147',\n",
      "                    'state': 'VA'}}\n"
     ]
    }
   ],
   "source": [
    "ip_string = \"72.68.212.63\"\n",
    "pp.pprint(xget_arin(ip_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dmv_test]",
   "language": "python",
   "name": "conda-env-dmv_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
